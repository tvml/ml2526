<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.45">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>loss</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="loss_files/libs/clipboard/clipboard.min.js"></script>
<script src="loss_files/libs/quarto-html/quarto.js"></script>
<script src="loss_files/libs/quarto-html/popper.min.js"></script>
<script src="loss_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="loss_files/libs/quarto-html/anchor.min.js"></script>
<link href="loss_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="loss_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="loss_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="loss_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="loss_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Rischio e minimizzazione</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a href="https://colab.research.google.com/github/tvml/ml2324/blob/master/codici/loss.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></p>
<div id="4a91b865" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> scipy</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy.special <span class="im">as</span> sp</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="754c9973" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">"xkcd:dusty blue"</span>, <span class="st">"xkcd:dark peach"</span>, <span class="st">"xkcd:dark seafoam green"</span>, </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>          <span class="st">"xkcd:dusty purple"</span>,<span class="st">"xkcd:watermelon"</span>, <span class="st">"xkcd:dusky blue"</span>, <span class="st">"xkcd:amber"</span>, </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>          <span class="st">"xkcd:purplish"</span>, <span class="st">"xkcd:dark teal"</span>, <span class="st">"xkcd:orange"</span>, <span class="st">"xkcd:slate"</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'ggplot'</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.family'</span>] <span class="op">=</span> <span class="st">'sans-serif'</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.serif'</span>] <span class="op">=</span> <span class="st">'Ubuntu'</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.monospace'</span>] <span class="op">=</span> <span class="st">'Ubuntu Mono'</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'font.size'</span>] <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.labelsize'</span>] <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.labelweight'</span>] <span class="op">=</span> <span class="st">'bold'</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'axes.titlesize'</span>] <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'xtick.labelsize'</span>] <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'ytick.labelsize'</span>] <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'legend.fontsize'</span>] <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.titlesize'</span>] <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'image.cmap'</span>] <span class="op">=</span> <span class="st">'jet'</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'image.interpolation'</span>] <span class="op">=</span> <span class="st">'none'</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">16</span>, <span class="dv">8</span>)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'lines.linewidth'</span>] <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'lines.markersize'</span>] <span class="op">=</span> <span class="dv">8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="2cbddc4a" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> urllib.request</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>filepath <span class="op">=</span> <span class="st">"../dataset/"</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://tvml.github.io/ml2324/dataset/"</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_file(filename, local):</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">#IS_COLAB = ('google.colab' in str(get_ipython()))</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> local:</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        urllib.request.urlretrieve (url<span class="op">+</span>filename, filename)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> filename</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> filepath<span class="op">+</span>filename</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="63e6afc8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_ds(data,m<span class="op">=</span><span class="va">None</span>,q<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    minx, maxx <span class="op">=</span> <span class="bu">min</span>(data.x1), <span class="bu">max</span>(data.x1)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    deltax <span class="op">=</span> <span class="fl">.1</span><span class="op">*</span>(maxx<span class="op">-</span>minx)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(minx<span class="op">-</span>deltax,maxx<span class="op">+</span>deltax,<span class="dv">1000</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.gca()</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    ax.scatter(data[data.t<span class="op">==</span><span class="dv">0</span>].x1, data[data.t<span class="op">==</span><span class="dv">0</span>].x2, s<span class="op">=</span><span class="dv">40</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">.7</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    ax.scatter(data[data.t<span class="op">==</span><span class="dv">1</span>].x1, data[data.t<span class="op">==</span><span class="dv">1</span>].x2, s<span class="op">=</span><span class="dv">40</span>, edgecolor<span class="op">=</span><span class="st">'k'</span>, alpha<span class="op">=</span><span class="fl">.7</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> m:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        ax.plot(x, m<span class="op">*</span>x<span class="op">+</span>q, lw<span class="op">=</span><span class="dv">2</span>, color<span class="op">=</span>colors[<span class="dv">5</span>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'$x_1$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'$x_2$'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Dataset'</span>, fontsize<span class="op">=</span><span class="dv">12</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="471ea714" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_1(cost_history, m, q, low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="va">None</span>, step<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> high <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        high <span class="op">=</span> m.shape[<span class="dv">0</span>]</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="bu">range</span>(low,high,step)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    ch <span class="op">=</span> cost_history[idx]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    th1 <span class="op">=</span> m[idx]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    th0 <span class="op">=</span> q[idx]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    minx, maxx, miny, maxy <span class="op">=</span> <span class="dv">0</span>, <span class="bu">len</span>(ch), ch.<span class="bu">min</span>(), ch.<span class="bu">max</span>()</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    deltay, deltax <span class="op">=</span> <span class="fl">.1</span><span class="op">*</span>(maxy<span class="op">-</span>miny), <span class="fl">.1</span><span class="op">*</span>(maxx<span class="op">-</span>minx)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    miny, maxy, minx, maxx <span class="op">=</span> miny <span class="op">-</span> deltay, maxy <span class="op">+</span> deltay, minx <span class="op">-</span> deltax, maxx <span class="op">+</span> deltax</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="bu">range</span>(<span class="bu">len</span>(ch)), ch, alpha<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>colors[<span class="dv">0</span>], linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'iterations'</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'cost'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    plt.xlim(minx,maxx)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    plt.ylim(miny,maxy)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(<span class="kw">lambda</span> x, pos: <span class="st">'</span><span class="sc">{:0.0f}</span><span class="st">'</span>.<span class="bu">format</span>(x<span class="op">*</span>step<span class="op">+</span>low)))</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    ax.set_yticklabels([]) </span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="02fd6bba" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_2(cost_history, m, q, low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="va">None</span>, step<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> high <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        high <span class="op">=</span> m.shape[<span class="dv">0</span>]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="bu">range</span>(low, high ,step)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    ch <span class="op">=</span> cost_history[idx]</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    th1 <span class="op">=</span> m[idx]</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    th0 <span class="op">=</span> q[idx]</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>,<span class="dv">6</span>))</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    minx, maxx, miny, maxy <span class="op">=</span> th0.<span class="bu">min</span>(), th0.<span class="bu">max</span>(), th1.<span class="bu">min</span>(), th1.<span class="bu">max</span>()</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    deltay, deltax <span class="op">=</span> <span class="fl">.1</span><span class="op">*</span>(maxy<span class="op">-</span>miny), <span class="fl">.1</span><span class="op">*</span>(maxx<span class="op">-</span>minx)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    miny, maxy <span class="op">=</span> miny <span class="op">-</span> deltay, maxy <span class="op">+</span> deltay</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    miny, maxy, minx, maxx <span class="op">=</span> miny <span class="op">-</span> deltay, maxy <span class="op">+</span> deltay, minx <span class="op">-</span> deltax, maxx <span class="op">+</span> deltax</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    ax.plot(th0, th1, alpha<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>colors[<span class="dv">1</span>], linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    ax.scatter(th0[<span class="op">-</span><span class="dv">1</span>],th1[<span class="op">-</span><span class="dv">1</span>], color<span class="op">=</span>colors[<span class="dv">5</span>], marker<span class="op">=</span><span class="st">'o'</span>, s<span class="op">=</span><span class="dv">40</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="vs">r'$x_1$'</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="vs">r'$x_2$'</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    plt.xlim(minx,maxx)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    plt.ylim(miny,maxy)</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    ax.grid(<span class="va">True</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    ax.set_xticklabels([])      </span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    ax.set_yticklabels([]) </span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="fc875422" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_all(cost_history, m, q, low<span class="op">=</span><span class="dv">0</span>, high<span class="op">=</span><span class="va">None</span>, step<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> high <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        high <span class="op">=</span> m.shape[<span class="dv">0</span>]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> <span class="bu">range</span>(low,high,step)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    ch <span class="op">=</span> cost_history[idx]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    th1 <span class="op">=</span> m[idx]</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    th0 <span class="op">=</span> q[idx]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">6</span>))</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    minx, maxx, miny, maxy <span class="op">=</span> <span class="dv">0</span>, <span class="bu">len</span>(ch), ch.<span class="bu">min</span>(), ch.<span class="bu">max</span>()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    deltay, deltax <span class="op">=</span> <span class="fl">.1</span><span class="op">*</span>(maxy<span class="op">-</span>miny), <span class="fl">.1</span><span class="op">*</span>(maxx<span class="op">-</span>minx)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    miny, maxy, minx, maxx <span class="op">=</span> miny <span class="op">-</span> deltay, maxy <span class="op">+</span> deltay, minx <span class="op">-</span> deltax, maxx <span class="op">+</span> deltax</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    ax.plot(<span class="bu">range</span>(<span class="bu">len</span>(ch)), ch, alpha<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>colors[<span class="dv">0</span>], linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'iterations'</span>)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'cost'</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    plt.xlim(minx,maxx)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    plt.ylim(miny,maxy)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    ax.xaxis.set_major_formatter(mpl.ticker.FuncFormatter(<span class="kw">lambda</span> x, pos: <span class="st">'</span><span class="sc">{:0.0f}</span><span class="st">'</span>.<span class="bu">format</span>(x<span class="op">*</span>step<span class="op">+</span>low)))</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> fig.add_subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    minx, maxx, miny, maxy <span class="op">=</span> th0.<span class="bu">min</span>(), th0.<span class="bu">max</span>(), th1.<span class="bu">min</span>(), th1.<span class="bu">max</span>()</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    deltay, deltax <span class="op">=</span> <span class="fl">.1</span><span class="op">*</span>(maxy<span class="op">-</span>miny), <span class="fl">.1</span><span class="op">*</span>(maxx<span class="op">-</span>minx)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    miny, maxy <span class="op">=</span> miny <span class="op">-</span> deltay, maxy <span class="op">+</span> deltay</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    miny, maxy, minx, maxx <span class="op">=</span> miny <span class="op">-</span> deltay, maxy <span class="op">+</span> deltay, minx <span class="op">-</span> deltax, maxx <span class="op">+</span> deltax</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>    ax.plot(th0, th1, alpha<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span>colors[<span class="dv">1</span>], linewidth<span class="op">=</span><span class="dv">2</span>, zorder<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    ax.scatter(th0[<span class="op">-</span><span class="dv">1</span>],th1[<span class="op">-</span><span class="dv">1</span>], color<span class="op">=</span>colors[<span class="dv">5</span>], marker<span class="op">=</span><span class="st">'o'</span>, s<span class="op">=</span><span class="dv">40</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="vs">r'$m$'</span>)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="vs">r'$q$'</span>)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    plt.xlim(minx,maxx)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    plt.ylim(miny,maxy)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    plt.xticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    plt.yticks(fontsize<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Dato un qualunque algoritmo che fornisce per ogni valore di input <span class="math inline">\(x\)</span> una previsione <span class="math inline">\(f(x)\)</span>, la qualità delle previsioni fornite dall’algoritmo può essere definita per mezzo di una <em>funzione di costo</em> (loss function) <span class="math inline">\(L(x_1, x_2)\)</span>, dove <span class="math inline">\(x_1\)</span> è il valore predetto dal modello e <span class="math inline">\(x_2\)</span> è il valore corretto associato a <span class="math inline">\(x\)</span> . Sostanzialmente, il valore della funzione di costo <span class="math inline">\(L(f(x),y)\)</span> misura quindi quanto “costa” (secondo il modello di costo indotto dalla funzione stessa) prevedere, dato <span class="math inline">\(x\)</span>, il valore <span class="math inline">\(f(x)\)</span> invece del valore corretto <span class="math inline">\(y\)</span>.</p>
<p>Dato che evidentemente il costo è dipendente dalla coppia di valori <span class="math inline">\(x,y\)</span>, una valutazione complessiva della qualità delle predizioni dell’algoritmo potrà essere fornita considerando il valore atteso della funzione di costo al variare di <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, nell’ipotesi di una (densità di) distribuzione di probabilità congiunta di tali valori <span class="math inline">\(p(x,y)\)</span>. La distribuzione <span class="math inline">\(p(x,y)\)</span> ci fornisce quindi la probabilità che il prossimo punto su cui effettuare la predizione sia <span class="math inline">\(x\)</span> e che il valore corretto da predire sia <span class="math inline">\(y\)</span>. Si noti che non si fa l’ipotesi che due diverse occorrenze di <span class="math inline">\(x\)</span> siano associate allo stesso valore di <span class="math inline">\(y\)</span>: non si assume quindi una relazione funzionale, seppure sconosciuta, tra <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, ma solo una relazione in probabilità <span class="math inline">\(p(y\mid x)\)</span>. Questo permette di considerare la presenza di rumore nelle osservazioni effettuate.</p>
<p>Da quanto detto, indicando con <span class="math inline">\(D_x\)</span> e <span class="math inline">\(D_y\)</span> i domini di definizione di <span class="math inline">\(x\)</span> e <span class="math inline">\(y\)</span>, e assunta una distribuzione <span class="math inline">\(p(x,y)\)</span> che fornisce un modello statistico del contesto in cui si intende effettuare le predizioni, la qualità di un algoritmo di previsione che calcola la funzione <span class="math inline">\(f(x)\)</span> sarà data dal <em>rischio</em> <span class="math display">\[
\mathcal{R}(f)=\mathbb{E}_p[L(f(x),y)]=\int_{D_x}\int_{D_y} L(f(x),y)p(x,y)dxdy
\]</span></p>
<p>Il rischio di dice quindi quanto ci aspettiamo che ci costi prevedere <span class="math inline">\(f(x)\)</span>, assumendo che:</p>
<ol type="1">
<li><p><span class="math inline">\(x\)</span> sia estratto a caso dalla distribuzione marginale <span class="math display">\[
p(x)=\int_{D_y} p(x,y)dy
\]</span></p></li>
<li><p>il relativo valore corretto da predire sia estratto a caso dalla distribuzione condizionata <span class="math display">\[
p(y\mid x)=\frac{p(x,y)}{p(x)}
\]</span></p></li>
<li><p>il costo sia rappresentato dalla funzione <span class="math inline">\(L(x_1,x_2)\)</span></p></li>
</ol>
<section id="esempio" class="level5">
<h5 class="anchored" data-anchor-id="esempio">Esempio</h5>
<p>Consideriamo il caso in cui vogliamo effettuare previsioni sulla possibilità di pioggia in giornata, date le condizioni del cielo al mattino, assumendo che le possibili osservazioni siano “sereno” (S), “nuvoloso” (N), “coperto” (C), e che le previsioni siano “pioggia” (T) e “non pioggia” (F). La funzione di costo, sarà allora del tipo <span class="math inline">\(L:\{T,F\}^2\mapsto\mathbb{R}\)</span></p>
<p>La definizione di una particolare funzione di costo è legata alla valutazione delle priorità dell’utente. Nel caso specifico, se si valuta allo stesso modo “sgradevole” uscire con l’ombrello (per una previsione T) senza poi doverlo usare che bagnarsi per la pioggia non avendo preso l’ombrello (per una previsione F) allora la funzione di costo risulta <span class="math inline">\(L_1(y,t)\)</span>, definita dalla tabella seguente</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x_1\)</span>/<span class="math inline">\(x_2\)</span></th>
<th style="text-align: center;">T</th>
<th style="text-align: center;">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">T</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">F</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Se invece reputiamo molto più sgradevole bagnarci per non aver preso l’ombrello rispetto a prendere l’ombrello stesso inutilmente, allora la funzione di costo <span class="math inline">\(L_2(y,t)\)</span>, potrà essere definita come</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x_1\)</span>/<span class="math inline">\(x_2\)</span></th>
<th style="text-align: center;">T</th>
<th style="text-align: center;">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">T</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td style="text-align: center;">F</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">0</td>
</tr>
</tbody>
</table>
<p>Se facciamo l’ipotesi che la distribuzione congiunta su <span class="math inline">\(\{S,N,C\}\times\{T,F\}\)</span> sia</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span>/<span class="math inline">\(y\)</span></th>
<th style="text-align: center;">T</th>
<th style="text-align: center;">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">S</td>
<td style="text-align: center;">.05</td>
<td style="text-align: center;">.2</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">.25</td>
<td style="text-align: center;">.25</td>
</tr>
<tr class="odd">
<td style="text-align: center;">C</td>
<td style="text-align: center;">.2</td>
<td style="text-align: center;">.05</td>
</tr>
</tbody>
</table>
<p>e consideriamo due possibili funzioni predittive <span class="math inline">\(f_1(x)\)</span> e <span class="math inline">\(f_2(x)\)</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 64%">
<col style="width: 23%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span></th>
<th style="text-align: center;"><span class="math inline">\(f_1(x)\)</span></th>
<th style="text-align: center;"><span class="math inline">\(f_2(x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">S</td>
<td style="text-align: center;">&nbsp; &nbsp; F &nbsp; &nbsp; &nbsp;</td>
<td style="text-align: center;">&nbsp; &nbsp; F &nbsp; &nbsp; &nbsp;</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">&nbsp; &nbsp; F</td>
<td style="text-align: center;">&nbsp; &nbsp; T</td>
</tr>
<tr class="odd">
<td style="text-align: center;">C</td>
<td style="text-align: center;">&nbsp; &nbsp; T</td>
<td style="text-align: center;">&nbsp; &nbsp; T</td>
</tr>
</tbody>
</table>
<p>possiamo verificare che nel caso in cui la funzione di costo sia <span class="math inline">\(L_1\)</span> allora il rischio nei due casi è <span class="math inline">\(\mathcal{R}(f_1)=0.65\)</span> e <span class="math inline">\(\mathcal{R}(f_2)=0.4\)</span> per cui <span class="math inline">\(f_2\)</span> è preferibile a <span class="math inline">\(f_1\)</span>. Al contrario, se la funzione di costo è <span class="math inline">\(L_2\)</span>, allora risulta <span class="math inline">\(\mathcal{R}(f_1)=1.55\)</span> e <span class="math inline">\(\mathcal{R}(f_2)=7.55\)</span>, per cui, al contrario, <span class="math inline">\(f_1\)</span> è preferibile a <span class="math inline">\(f_2\)</span>.</p>
<p>Come si vede, quindi, la scelta tra <span class="math inline">\(f_1(x)\)</span> e <span class="math inline">\(f_2(x)\)</span> è dipendente dalla funzione di costo adottata e dalla distribuzione <span class="math inline">\(p(x,y)\)</span> che invece è data e, tra l’altro, sconosciuta. Quindi, una diversa distribuzione potrebbe portare a conclusioni diverse anche considerando una stessa funzione di costo: se ad esempio si fa riferimento alla funzione di costo <span class="math inline">\(L_1\)</span>, allora la distribuzione congiunta</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x\)</span>/<span class="math inline">\(y\)</span></th>
<th style="text-align: center;">T</th>
<th style="text-align: center;">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">S</td>
<td style="text-align: center;">.05</td>
<td style="text-align: center;">.05</td>
</tr>
<tr class="even">
<td style="text-align: center;">N</td>
<td style="text-align: center;">.05</td>
<td style="text-align: center;">.4</td>
</tr>
<tr class="odd">
<td style="text-align: center;">C</td>
<td style="text-align: center;">.05</td>
<td style="text-align: center;">.4</td>
</tr>
</tbody>
</table>
<p>determina dei valori di rischio <span class="math inline">\(\mathcal{R}(f_1)=0.6\)</span> e <span class="math inline">\(\mathcal{R}(f_2)=0.9\)</span>, rendendo ora <span class="math inline">\(f_1\)</span> preferibile a <span class="math inline">\(f_2\)</span>.</p>
</section>
<section id="rischio-empirico" class="level4">
<h4 class="anchored" data-anchor-id="rischio-empirico">Rischio empirico</h4>
<p>Dato che la distribuzione reale <span class="math inline">\(p(x,y)\)</span> è sconosciuta per ipotesi (se così non fosse potremmo sempre effettuare predizioni utilizzando la distribuzione condizionata reale <span class="math inline">\(p(y\mid x)\)</span>) il calcolo del rischio reale è impossibile ed è necessario effettuare delle approssimazioni, sulla base dei dati disponibili. In particolare, possiamo applicare il metodo standard di utilizzando la media aritmetica su un campione come stimatore del valore atteso, e considerare il <em>rischio empirico</em> (empirical risk) calcolato effettuando l’operazione di media sul campione offerto dai dati disponibili nel training set <span class="math inline">\(X=\{(x_1,y_1),\ldots,(x_n,y_n)\}\)</span></p>
<p><span class="math display">\[
\overline{\mathcal{R}}(f; X)=\overline{L}(f(x), y; X)=\frac{1}{n}\sum_{i=1}^nL(f(x_i),y_i)
\]</span></p>
<p>La funzione utilizzata per le predizioni sarà allora quella che, nell’insieme di funzioni considerato, minimizza il rischio empirico</p>
<p><span class="math display">\[
f^*=\underset{f\in F}{\mathrm{argmin}}\;\overline{\mathcal{R}}(f;X)
\]</span></p>
<p>Si noti che, in effetti, il rischio empirico dipende sia dai dati in <span class="math inline">\(X\)</span> che dalla funzione <span class="math inline">\(f\)</span>: in questo senso è una funzione rispetto a <span class="math inline">\(X\)</span> e un funzionale rispetto a <span class="math inline">\(f\)</span>. La ricerca di <span class="math inline">\(f^*\)</span> comporta quindi una minimizzazione funzionale del rischio empirico. In generale, tale situazione viene semplificata limitando la ricerca all’interno di classi di funzioni definite da coefficienti: in questo modo, il rischio empirico può essere espresso come funzione dei coefficienti della funzione (oltre che di <span class="math inline">\(X\)</span>) e la minimizzazione è una normale minimizzazione di funzione.</p>
<p>Chiaramente, la speranza è che minimizzare il rischio empirico dia risultati simili a quelli che si otterrebbero minimizzando il rischio reale. Ciò dipende, in generale, da quattro fattori:</p>
<ul>
<li>La dimensione del training set <span class="math inline">\(X\)</span>. Al crescere della quantità di dati, <span class="math inline">\(\overline{\mathcal{R}}(f; X)\)</span> tende a <span class="math inline">\(\mathcal{R}(f)\)</span> per ogni funzione <span class="math inline">\(f\)</span></li>
<li>La distribuzione reale <span class="math inline">\(p(x,y)\)</span>. Maggiore è la sua complessità, maggiore è la quantità di dati necessari per averne una buona approssimazione.</li>
<li>La funzione di costo <span class="math inline">\(L\)</span>, che può creare problemi se assegna costi molto elevati in situazioni particolari e poco probabili</li>
<li>L’insieme <span class="math inline">\(F\)</span> delle funzioni considerate. Se la sua dimensione è elevata, e le funzioni hanno una struttura complessa, una maggior quantità di dati risulta necessaria per avere una buona approssimazione. Al tempo stesso, considerare un insieme piccolo di funzioni semplici rende sì la minimizzazione del rischio implicito su <span class="math inline">\(F\)</span> una buona approssimazione del minimo rischio reale su <span class="math inline">\(F\)</span> stesso, ma al tempo stesso comporta che tale minimo possa essere molto peggiore di quello ottenibile considerando classi più ampie di funzioni.#### Rischio empirico Dato che la distribuzione reale <span class="math inline">\(p(\mathbf{x},t)\)</span> è sconosciuta per ipotesi (se così non fosse potremmo sempre effettuare predizioni utilizzando la distribuzione condizionata reale <span class="math inline">\(p(t\mid \mathbf{x})\)</span>) il calcolo del rischio reale è impossibile ed è necessario effettuare delle approssimazioni, sulla base dei dati disponibili. In particolare, possiamo applicare il metodo standard di utilizzando la media aritmetica su un campione come stimatore del valore atteso, e considerare il <em>rischio empirico</em> (empirical risk) calcolato effettuando l’operazione di media sul campione offerto dai dati disponibili nel training set <span class="math inline">\(X=\{(\mathbf{x}_1,t_1),\ldots,(\mathbf{x}_n,t_n)\}\)</span></li>
</ul>
<p><span class="math display">\[
\overline{\mathcal{R}}(f; X)=\frac{1}{n}\sum_{i=1}^nL(f(x_i),y_i)
\]</span></p>
<p>La funzione utilizzata per le predizioni sarà allora quella che, nell’insieme di funzioni considerato, minimizza il rischio empirico</p>
<p><span class="math display">\[
f^*=\underset{f\in F}{\mathrm{argmin}}\;\overline{\mathcal{R}}(f;X)
\]</span></p>
<p>Si noti che, in effetti, il rischio empirico dipende sia dai dati in <span class="math inline">\(X\)</span> che dalla funzione <span class="math inline">\(f\)</span>: in questo senso è una funzione rispetto a <span class="math inline">\(X\)</span> e un funzionale rispetto a <span class="math inline">\(f\)</span>. La ricerca di <span class="math inline">\(f^*\)</span> comporta quindi una minimizzazione funzionale del rischio empirico. In generale, tale situazione viene semplificata limitando la ricerca all’interno di classi di funzioni definite da coefficienti: in questo modo, il rischio empirico può essere espresso come funzione dei coefficienti della funzione (oltre che di <span class="math inline">\(X\)</span>) e la minimizzazione è una normale minimizzazione di funzione.</p>
<p>Chiaramente, la speranza è che minimizzare il rischio empirico dia risultati simili a quelli che si otterrebbero minimizzando il rischio reale. Ciò dipende, in generale, da quattro fattori:</p>
<ul>
<li>La dimensione del training set <span class="math inline">\(X\)</span>. Al crescere della quantità di dati, <span class="math inline">\(\overline{\mathcal{R}}(f; X)\)</span> tende a <span class="math inline">\(\mathcal{R}(f)\)</span> per ogni funzione <span class="math inline">\(f\)</span></li>
<li>La distribuzione reale <span class="math inline">\(p(\mathbf{x},t)\)</span>. Maggiore è la sua complessità, maggiore è la quantità di dati necessari per averne una buona approssimazione.</li>
<li>La funzione di costo <span class="math inline">\(L\)</span>, che può creare problemi se assegna costi molto elevati in situazioni particolari e poco probabili</li>
<li>L’insieme <span class="math inline">\(F\)</span> delle funzioni considerate. Se la sua dimensione è elevata, e le funzioni hanno una struttura complessa, una maggior quantità di dati risulta necessaria per avere una buona approssimazione. Al tempo stesso, considerare un insieme piccolo di funzioni semplici rende sì la minimizzazione del rischio implicito su <span class="math inline">\(F\)</span> una buona approssimazione del minimo rischio reale su <span class="math inline">\(F\)</span> stesso, ma al tempo stesso comporta che tale minimo possa essere molto peggiore di quello ottenibile considerando classi più ampie di funzioni.</li>
</ul>
</section>
<section id="minimizzazione-della-funzione-di-rischio" class="level3">
<h3 class="anchored" data-anchor-id="minimizzazione-della-funzione-di-rischio">Minimizzazione della funzione di rischio</h3>
<p>In generale, l’insieme <span class="math inline">\(F\)</span> delle funzioni è definito in modo parametrico <span class="math inline">\(F=\{f(\mathbf{x};\theta)\}\)</span> dove <span class="math inline">\(\theta\in D_\theta\)</span> è un coefficiente (tipicamente multidimensionale) che determina, all’interno della classe <span class="math inline">\(F\)</span> (definita tipicamente in termini ‘’strutturali’’) la particolare funzione utilizzata. Un esempio tipico è offerto dalla <em>regressione lineare</em>, in cui si vuole prevedere il valore di un attributo <span class="math inline">\(y\)</span> con dominio <span class="math inline">\(R\)</span> sulla base dei valori di altri <span class="math inline">\(m\)</span> attributi <span class="math inline">\(\mathbf{x}=(x_1,\ldots, x_m)\)</span> (che assumiamo per semplicità in <span class="math inline">\(R\)</span> anch’essi): nella regressione lineare, l’insieme delle possibili funzioni <span class="math inline">\(f:R^m\mapsto R\)</span> è limitato alle sole funzioni lineari <span class="math inline">\(f_\mathbf{w}(\mathbf{x})=w_0+w_1x_1+\ldots+w_mx_m\)</span>, e il parametro <span class="math inline">\(\theta\)</span> corrisponde al vettore <span class="math inline">\(\mathbf{w}=(w_0,\ldots,w_m)\)</span> dei coefficienti.</p>
<p>In questo caso, il rischio empirico, fissata la famiglia <span class="math inline">\(F\)</span> di funzioni, può essere ora inteso come funzione di <span class="math inline">\(\theta\)</span> <span class="math display">\[
\overline{\mathcal{R}}(\theta; X)=\frac{1}{n}\sum_{i=1}^nL(f(\mathbf{x}_i;\theta),t_i)\hspace{2cm}f\in F
\]</span> e la minimizzazione del rischio empirico può essere effettuata rispetto a <span class="math inline">\(\theta\)</span> <span class="math display">\[
\theta^*=\underset{\theta\in D_\theta}{\mathrm{argmin}}\;\overline{\mathcal{R}}(\theta;X)
\]</span> da cui deriva la funzione ottima (nella famiglia <span class="math inline">\(F\)</span>) <span class="math inline">\(f^*=f(\mathbf{x};\theta^*)\)</span></p>
<p>la minimizzazione della funzione di rischio avrà luogo nel dominio di definizione <span class="math inline">\(D_\theta\)</span> di <span class="math inline">\(\theta\)</span>, e potrà essere effettuata in modi diversi, in dipendenza della situazione e di considerazioni di efficienza di calcolo e di qualità delle soluzioni derivate.</p>
<section id="ricerca-analitica-dellottimo" class="level4">
<h4 class="anchored" data-anchor-id="ricerca-analitica-dellottimo">Ricerca analitica dell’ottimo</h4>
<p>Se il problema si pone in termini di minimizzazione <em>senza vincoli</em>, e quindi all’interno di <span class="math inline">\(R^m\)</span>, un primo approccio è quello standard dell’analisi di funzioni, consistente nella ricerca di valori <span class="math inline">\(\overline\theta\)</span> di <span class="math inline">\(\theta\)</span> per i quali si annullano tutte le derivate parziali <span class="math inline">\(\frac{\partial \overline{\mathcal{R}}(\theta; X)}{\partial \theta_i}\)</span>, tale cioè che, se indichiamo con <span class="math inline">\(m\)</span> la dimensione (numero delle componenti) di <span class="math inline">\(\theta\)</span>, il sistema su <span class="math inline">\(m\)</span> incognite definito dalle <span class="math inline">\(m\)</span> equazioni <span class="math display">\[
\frac{\partial \overline{\mathcal{R}}(\theta; X)}{\partial \theta_i}\Bigr|_{\theta=\overline\theta}=0\hspace{2cm} i=1,\ldots,m
\]</span> risulta soddisfatto. La soluzione analitica di questo sistema risulta tipicamente ardua o impossibile, per cui vengono spesso adottate tecniche di tipo numerico.</p>
<section id="gradient-descent" class="level5">
<h5 class="anchored" data-anchor-id="gradient-descent">Gradient descent</h5>
<p>La discesa del gradiente (<em>gradient descent</em>) è una delle tecniche di ottimizzazione più popolari, in particolare nel settore del Machine Learning e delle Reti Neurali. La tecnica consiste nel minimizzare una funzione obiettivo <span class="math inline">\(J(\theta)\)</span> definita sui parametri <span class="math inline">\(\theta\in\mathbb{R}^d\)</span> del modello mediante aggiornamenti successivi del valore di <span class="math inline">\(\theta\)</span> (a partire da un valore iniziale <span class="math inline">\(\theta^{(0)}\)</span>) nella direzione opposta a quella del valore attuale del gradiente <span class="math inline">\(J'(\theta)=\nabla J(\theta)\)</span>. Si ricorda, a tale proposito, che, data una funzione <span class="math inline">\(f(x_1,x_2,\ldots,x_d)\)</span>, il gradiente <span class="math inline">\(\nabla f\)</span> di <span class="math inline">\(f\)</span> è il vettore <span class="math inline">\(d\)</span>-dimensionale delle derivate di <span class="math inline">\(f\)</span> rispetto alle variabili <span class="math inline">\(x_1,\ldots, x_d\)</span>: il vettore cioè tale che <span class="math inline">\([\nabla f]_i=\frac{\partial f}{\partial x_i}\)</span>. Un parametro <span class="math inline">\(\eta\)</span>, detto <em>learning rate</em> determina la scala degli aggiornamenti effettuati, e quindi la dimensione dei passi effettuati nella direzione di un minimo locale.</p>
<p>Possiamo interpretare la tecnica come il muoversi sulla superficie della funzione <span class="math inline">\(J(\theta)\)</span> seguendo sempre la direzione di massima pendenza verso il basso, fino a raggiungere un punto da cui è impossibile scendere ulteriormente.</p>
<p>Come si può osservare, in questo modo il metodo consente di raggiungere un valore di <span class="math inline">\(\theta\)</span> per il quale il gradiente risulta nullo, e quindi (nel contesto assunto) un minimo locale o un punto di sella della funzione di costo. In linea di principio,</p>
</section>
<section id="varianti-di-discesa-del-gradiente" class="level5">
<h5 class="anchored" data-anchor-id="varianti-di-discesa-del-gradiente">Varianti di discesa del gradiente</h5>
<p>In molti casi, e sempre nell’ambito del ML, la funzione obiettivo corrisponde all’applicazione di una funzione di costo (<em>loss function</em>), predefinita e dipendente dal modello adottato, su un insieme dato di elementi di un dataset <span class="math inline">\(X=(\mathbf{x}_1,\ldots, \mathbf{x}_n)\)</span> (che nel caso di apprendimento supervisionato è un insieme di coppie <span class="math inline">\(X=((\mathbf{x}_1,t_1),\ldots,(\mathbf{x}_n,t_n))\)</span>): rappresentiamo questa situazione con <span class="math inline">\(J(\theta; X)\)</span>. Questo corrisponde all’approssimazione del <em>rischio</em></p>
<p><span class="math display">\[
\mathcal{R}(\theta)=\int J(\theta,\mathbf{x})p(\mathbf{x})d\mathbf{x}=E_{p}[\theta]
\]</span></p>
<p>In generale, la funzione di costo è definita in modo additivo rispetto agli elementi di <span class="math inline">\(X\)</span> (il costo relativo all’insieme <span class="math inline">\(X\)</span> è pari alla somma dei costi relativi ai suoi elementi), per cui il valore risulta <span class="math inline">\(J(\theta;X)=\sum_{i=1}^nJ(\theta;\mathbf{x}_i)\)</span>, o preferibilmente, per evitare una eccessiva dipendenza dal numero di elementi, come media <span class="math display">\[
J(\theta;X)=\frac{1}{n}\sum_{i=1}^nJ(\theta;\mathbf{x}_i)
\]</span> Si noti che, per le proprietà dell’operazione di derivazione, da questa ipotesi deriva l’additività anche del gradiente, per cui <span class="math display">\[
J'(\theta; X)=\sum_{i=1}^nJ'(\theta;\mathbf{x}_i)
\]</span> o <span class="math display">\[
J'(\theta;X)=\frac{1}{n}\sum_{i=1}^nJ'(\theta;\mathbf{x}_i)
\]</span></p>
<p>Possiamo allora identificare tre varianti del metodo, che differiscono tra loro per la quantità di elementi di <span class="math inline">\(X\)</span> utilizzati, ad ogni passo, per calcolare il gradiente della funzione obiettivo. Una quantità maggiore di dati utilizzati aumenta l’accuratezza dell’aggiornamento, ma anche il tempo necessario per effettuare l’aggiornamento stesso (in particolare, per valutare il gradiente per il valore attuale di <span class="math inline">\(\theta\)</span>).</p>
<section id="batch-gradient-descent" class="level6">
<h6 class="anchored" data-anchor-id="batch-gradient-descent">Batch gradient descent</h6>
<p>In questo caso, il gradiente è valutato, ogni volta, considerando tutti gli elementi nel training set <span class="math inline">\(X\)</span>. Quindi si ha che al passo <span class="math inline">\(k\)</span>-esimo viene eseguito l’aggiornamento</p>
<p><span class="math display">\[
\theta^{(k+1)}=\theta^{(k)}-\eta\sum_{i=1}^nJ'(\theta^{(k)};\mathbf{x}_i)
\]</span></p>
<p>o anche, per i singoli coefficienti</p>
<p><span class="math display">\[
\theta_j^{(k+1)}=\theta_j^{(k)}-\eta\sum_{i=1}^n\frac{\partial J(\theta;\mathbf{x}_i)}{\partial\theta_j}\Bigr\vert_{\small\theta=\theta^{(k)}}
\]</span></p>
<p>Dato che si richiede quindi, ad ogni iterazione, la valutazione del gradiente (con il valore attuale <span class="math inline">\(\theta^{(k)}\)</span> di tutti i coefficienti) su tutti gli elementi di <span class="math inline">\(X\)</span>, questa soluzione tende ad essere molto lenta, soprattutto in presenza di dataset di dimensioni molto estese, come nel caso di reti neurali complesse e deep learning. Inoltre, l’approccio diventa del tutto impraticabile se il dataset è talmente esteso da non entrare neanche in memoria.</p>
<p>In termini di codice, il metodo batch gradient descent si presenta come:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(dataset_size):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> g<span class="op">+</span>evaluate_gradient(loss_function, theta, X[k])</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta<span class="op">-</span>eta<span class="op">*</span>g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Il ciclo viene eseguito un numero di volte pari al numero di epoche, dove per <em>epoca</em> si intende una iterazione su tutti gli elementi di <span class="math inline">\(X\)</span>. Di conseguenza, la valutazione di <span class="math inline">\(\theta\)</span> viene aggiornata un numero di volte pari al numero di epoche. Il metodo batch gradient descent converge certamente al minimo globale se la funzione <span class="math inline">\(J(\theta)\)</span> è convessa, mentre altrimenti converge a un minimo locale.</p>
</section>
</section>
<section id="esempio-1" class="level5">
<h5 class="anchored" data-anchor-id="esempio-1">Esempio</h5>
<p>Applichiamo le considerazioni a un semplice problema di classificazione su un dataset bidimensionale, riportato graficamente di seguito.</p>
<div id="00f29174" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.read_csv(get_file(<span class="st">"testSet.txt"</span>, local<span class="op">=</span><span class="va">False</span>), <span class="op">\</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    delim_whitespace<span class="op">=</span><span class="va">True</span>, header<span class="op">=</span><span class="va">None</span>, names<span class="op">=</span>[<span class="st">'x1'</span>,<span class="st">'x2'</span>,<span class="st">'t'</span>])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plot_ds(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-9-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="7a816d0a" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>nfeatures <span class="op">=</span> <span class="bu">len</span>(data.columns)<span class="op">-</span><span class="dv">1</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array(data[[<span class="st">'x1'</span>,<span class="st">'x2'</span>]])</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> np.array(data[<span class="st">'t'</span>]).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.column_stack((np.ones(n), X))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il metodo considerato per la classificazione è la <em>logistic regression</em>, che determina un iperpiano (retta, in questo caso) di separazione minimizzando rispetto al vettore <span class="math inline">\(\theta\)</span> dei coefficienti dell’equazione dell’iperpiano (3 in questo caso) il rischio empirico sul dataset associato alla funzione di costo <em>cross-entropy</em>, per la quale il costo associato a un singolo elemento <span class="math inline">\(\mathbf{x}=(x_1,\ldots,x_d)\)</span> è</p>
<p><span class="math display">\[ J(\theta, \mathbf{x})=-\left(t\log y + (1-t)\log (1-y)\right) \]</span> dove <span class="math inline">\(t\)</span> è il valore <em>target</em> è il valore <span class="math inline">\(0/1\)</span> della classe dell’elemento e <span class="math inline">\(y\in (0,1)\)</span> è il valore predetto dal modello, definito come <span class="math display">\[
y = \sigma(\theta; \mathbf{x}) = \frac{1}{1+e^{-\sum_{i=1}^d\theta_ix_i+\theta_0}}
\]</span></p>
<div id="8b7e61bf" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigma(theta, X):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> sp.expit(np.dot(X, theta)).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il rischio empirico associato all’intero dataset può essere allora definito come la corrispondente media <span class="math display">\[
J(\theta, X)=\frac{1}{n}\sum_{i=1}^n \left(t_i\log \sigma(\mathbf{x}_i) -(1-t_i)\log (1-\sigma(\mathbf{x}_i))\right)
\]</span></p>
<div id="4a2bf512" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> approx_zero(v):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">1e-50</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    v[v<span class="op">&lt;</span>eps]<span class="op">=</span>eps</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> v</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cost(theta, X, t):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    eps <span class="op">=</span> <span class="fl">1e-50</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> sigma(theta,X)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    v[v<span class="op">&lt;</span>eps]<span class="op">=</span>eps</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    term1 <span class="op">=</span> np.dot(np.log(v).T,t)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> <span class="fl">1.0</span> <span class="op">-</span> sigma(theta,X)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    v[v<span class="op">&lt;</span>eps]<span class="op">=</span>eps</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    term2 <span class="op">=</span> np.dot(np.log(v).T,<span class="dv">1</span><span class="op">-</span>t)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> ((<span class="op">-</span>term1 <span class="op">-</span> term2) <span class="op">/</span> <span class="bu">len</span>(X))[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Il gradiente della funzione di costo risulta allora pari a</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial J(\theta,x)}{\partial\theta_i}&amp;=-(t-\sigma(x;\theta))x_i\hspace{1cm}i=1,\ldots,d\\
\frac{\partial J(\theta,x)}{\partial\theta_0}&amp;=-(t-\sigma(x;\theta))
\end{align*}\]</span></p>
<p>e il corrispondente gradiente del rischio empirico è dato da</p>
<p><span class="math display">\[\begin{align*}
\frac{\partial J(\theta,X)}{\partial\theta_i}&amp;=-\frac{1}{n}\sum_{j=1}^n (t_j-\sigma(x_j;\theta))x_{ji}\hspace{1cm}i=1,\ldots,d\\
\frac{\partial J(\theta,X)}{\partial\theta_0}&amp;=-\frac{1}{n}\sum_{i=1}^n(t_j-\sigma(x_j;\theta))
\end{align*}\]</span></p>
<div id="63a384be" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient(theta, X, t):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.dot(X.T, (t<span class="op">-</span>sigma(theta, X))) <span class="op">/</span> <span class="bu">len</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Per quanto detto, una iterazione di BGD corrisponde agli aggiornamenti</p>
<p><span class="math display">\[\begin{align*}
\theta_j^{(k+1)}&amp;=\theta_j^{(k)}-\eta\frac{\partial J(\theta,X)}{\partial\theta_j}{\LARGE\vert}_{\small\theta=\theta^{(k)}}=\theta_j^{(k)}+\frac{\eta}{n}\sum_{i=1}^n (t_i-\sigma(x_i;\theta^{(k)}))x_{ij}\hspace{1cm}j=1,\ldots,d\\
\theta_0^{(k+1)}&amp;=\theta_0^{(k)}-\eta\frac{\partial J(\theta,X)}{\partial\theta_0}{\LARGE\vert}_{\small\theta=\theta^{(k)}}=\theta_0^{(k)}+\frac{\eta}{n}\sum_{i=1}^n(t_i-\sigma(x_i;\theta^{(k)}))
\end{align*}\]</span></p>
<div id="304ecf12" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_gd(X, t, eta <span class="op">=</span> <span class="fl">0.1</span>, epochs <span class="op">=</span> <span class="dv">10000</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):    </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> <span class="op">-</span> eta <span class="op">*</span> gradient(theta,X,t)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> delta</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, theta_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Applicando il metodo sul dataset, fissando un valore per il parametro <span class="math inline">\(\eta\)</span> e per il numero di epoche (dove una epoca corrisponde all’applicazione dell’iterazione su tutti gli elementi del dataset), otteniamo le sequenze dei costi e dei valori di coefficiente angolare e termine noto della retta di separazione.</p>
<div id="e4539d07" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>cost_history, theta_history, m, q <span class="op">=</span> batch_gd(X, t, eta <span class="op">=</span> <span class="fl">0.1</span>, epochs <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi"</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti calcolati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.338 secondi
10000 passi
1000000 gradienti calcolati</code></pre>
</div>
</div>
<div id="7fa27c9b" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>m_star <span class="op">=</span> m[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>q_star <span class="op">=</span> q[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"valori ottimi: m=</span><span class="sc">{</span>m_star<span class="sc">:4.5f}</span><span class="ss">, q=</span><span class="sc">{</span>q_star<span class="sc">:4.5f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>valori ottimi: m=0.65705, q=7.31392</code></pre>
</div>
</div>
<p>La convergenza regolare del metodo è evidente nella figura seguente, dove si mostrano un andamento tipico della funzione di costo rispetto al numero di iterazioni e la sequenza di valori assunti da <span class="math inline">\(\theta\)</span>, considerata bidimensionale.</p>
<div id="8c4a25c1" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-17-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ace60a46" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>plot_1(cost_history, m, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-18-output-1.png" width="856" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="1ff0c341" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>plot_2(cost_history, m, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-19-output-1.png" width="854" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="e95d9e0a" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#m_star = 0.62595499</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co">#q_star = 7.3662299</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> i: np.sqrt((m_star<span class="op">-</span>m[i])<span class="op">**</span><span class="dv">2</span><span class="op">+</span>(q_star<span class="op">-</span>q[i])<span class="op">**</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5af08dec" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    dist <span class="op">=</span> np.array([f(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(m))])</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    near <span class="op">=</span> np.where(dist<span class="op">&lt;</span>min_dist)</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(near[<span class="dv">0</span>])<span class="op">&gt;</span><span class="dv">0</span>:</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        ll <span class="op">=</span> np.<span class="bu">min</span>(near[<span class="dv">0</span>])</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>ll<span class="sc">}</span><span class="ss"> passi e </span><span class="sc">{</span>ll<span class="op">*</span>ge_per_step<span class="sc">}</span><span class="ss"> gradienti calcolati per arrivare a distanza minore o uguale di </span><span class="sc">{</span>min_dist<span class="sc">}</span><span class="ss"> dall'ottimo"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"in </span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi e </span><span class="sc">{</span>(<span class="bu">len</span>(m))<span class="op">*</span>ge_per_step<span class="sc">}</span><span class="ss"> gradienti calcolati non raggiunge un punto a distanza minore o uguale di </span><span class="sc">{</span>min_dist<span class="sc">}</span><span class="ss"> dall'ottimo"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="7345e8a1" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3767 passi e 376700 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="b17fb30f" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m_star,q_star)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-23-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="stochastic-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic gradient descent</h4>
<p>Nella stochastic gradient descent, a differenza del caso precedente, la valutazione del gradiente effettuata ad ogni iterazione fa riferimento a un solo elemento <span class="math inline">\(x_i\)</span> del training set. Quindi si ha</p>
<p><span class="math display">\[
\theta^{(k+1)}=\theta^{(k)}-\eta J'(\theta^{(k)};x_i)
\]</span></p>
<p>e, per i singoli coefficienti,</p>
<p><span class="math display">\[
\theta_j^{(k+1)}=\theta_j^{(k)}-\eta\frac{\partial J(\theta;x_i)}{\partial\theta_j}\LARGE\vert_{\small\theta=\theta^{(k)}}
\]</span></p>
<p>La discesa del gradiente batch valuta il gradiente per tutti gli elementi, anche quelli simili tra loro, a ogni iterazione, eseguendo così un insieme ridondante di operazioni. SGD risolve questo problema effettuando una sola valutazione, e quindi operando in modo più veloce.</p>
<p>Al tempo stesso, però, mentre i valori della funzione di costo nel caso di BGD decrescono con regolarità verso il minimo locale, applicando SGD si riscontra un andamento molto più irregolare, con fluttuazione della funzione di costo intorno a un trend complessivo di decrescita, ma con incrementi locali anche significativi. Questo da un lato può non risultare negativo, in quanto le oscillazioni locali posso consentire di uscire dall’intorno di un minimo locale, proseguendo la ricerca di nuovi minimi. Al tempo stesso, l’oscillazione locale rende difficile la convergenza finale verso il minimo.</p>
<p>Questa oscillazione si riscontra anche nell’andamento dei valori dei coefficienti. Si noti comunque che, considerando la sequenza dei valori della funzione di costo assunti al termine di ogni <em>epoca</em> (sequenza delle iterazioni che considerano tutti gli elementi del dataset), emerge la tendenza di decrescita di fondo.</p>
<p>In termini di codice, il metodo stochastic gradient descent si presenta come:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(data)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(dataset_size):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> evaluate_gradient(loss_function, theta, X[k])</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta<span class="op">-</span>eta<span class="op">*</span>g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Nel caso della logistic regression, l’aggiornamento a ogni iterazione risulta quindi</p>
<p><span class="math display">\[\begin{align*}
\theta_j^{(k+1)}&amp;=\theta_j^{(k)}+\eta(t_i-\sigma(x_i))x_{ij}\hspace{1cm}j=1,\ldots,d\\
\theta_0^{(k+1)}&amp;=\theta_0^{(k)}+\eta(t_i-\sigma(x_i))
\end{align*}\]</span></p>
<div id="8f1ea8cd" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stochastic_gd(X, t, eta <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>            e <span class="op">=</span> (t[i] <span class="op">-</span> sigma(theta, X[i,:]))[<span class="dv">0</span>]</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> theta <span class="op">+</span> eta <span class="op">*</span> e <span class="op">*</span> X[i,:].reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>            theta_history.append(theta)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>            cost_history.append(cost(theta, X, t))</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, theta_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Applicando il metodo è necessario ancora specificare il valore di <span class="math inline">\(\theta\)</span> e il numero di epoche. Per la struttura dell’algoritmo, si avranno allora un numero di iterazioni pari al numero di epoche moltiplicato per la dimensionae <span class="math inline">\(n\)</span> del dataset.</p>
<div id="4158ff05" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>cost_history, theta_history, m, q <span class="op">=</span> stochastic_gd(X, t, eta <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">5000</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi"</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> gradienti calcolati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  16.074 secondi
500000 passi
500000 gradienti calcolati</code></pre>
</div>
</div>
<div id="775875ea" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, high<span class="op">=</span><span class="dv">1000</span>, step<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-26-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="275b954c" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">#plot_1(cost_history, m, q, low, high, step)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="79800b47" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co">#plot_2(cost_history, m, q, low, high, step)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="ddc19fae" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>12835 passi e 12835 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="9cfc5f5e" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m[<span class="op">-</span><span class="dv">1</span>],q[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-30-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Come si può vedere dalla figura seguente, considerando i valori di costo e dei coefficienti soltanto alla fine delle varie epoche risulta una andamento uniforme dei valori stessi.</p>
<div id="d4353f40" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>high, step <span class="op">=</span> <span class="dv">1000</span><span class="op">*</span>n, n</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, high<span class="op">=</span>high, step<span class="op">=</span>step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-31-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="mini-batch-gradient-descent" class="level4">
<h4 class="anchored" data-anchor-id="mini-batch-gradient-descent">Mini-batch gradient descent</h4>
<p>Questo approccio si pone in posizione intermedia rispetto ai due precedenti, generalizzando l’impostazione di SGD di considerare un solo elemento per iterazione a considerare sottoinsiemi diversi del dataset. L’algoritmo opera quindi partizionando, all’inizio di ogni epoca, il dataset in <span class="math inline">\(\lceil n/s\rceil\)</span> sottoinsiemi (<em>mini-batch</em>) di dimensione prefissata <span class="math inline">\(s\)</span>, ed effettuando poi <span class="math inline">\(\lceil n/s\rceil\)</span> iterazioni all’interno di ognuna delle quali l’aggiornamento di <span class="math inline">\(\theta\)</span> viene effettuato valutando il gradiente sugli <span class="math inline">\(s\)</span> elementi del mini-batch attuale.</p>
<p>La discesa del gradiente con mini-batch è l’algoritmo tipicamente utilizzato per l’addestramento di reti neurali, in particolare in presenza di reti <em>deep</em>.</p>
<p>Se indichiamo con <span class="math inline">\(X_i\subset X\)</span> il mini-batch attualmente considerato, l’aggiornamento a ogni iterazione è il seguente <span class="math display">\[
\theta^{(k+1)}=\theta^{(k)}-\eta\sum_{\mathbf{x}\in X_i}J'(\theta^{(k)};\mathbf{x})
\]</span> o anche <span class="math display">\[
\theta_j^{(k+1)}=\theta_j^{(k)}-\eta\sum_{\mathbf{x}\in X_i}\frac{\partial J(\theta;\mathbf{x})}{\partial\theta_j}\LARGE\vert_{\small\theta=\theta^{(k)}}
\]</span> In questo modo, la varianza degli aggiornamenti dei coefficienti viene diminuita. Inoltre, è possibile fare uso, in pratica, di implementazioni molto efficienti del calcolo del gradiente rispetto a un mini-batch disponibili nelle più recenti librerie per il <em>deep learning</em>. La dimensione dei mini-batch varia tra <span class="math inline">\(50\)</span> e <span class="math inline">\(256\)</span>.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(data)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> get_batches(dataset, batch_size):</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> batch:</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> g<span class="op">+</span>evaluate_gradient(loss_function, theta, batch)</span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta<span class="op">-</span>eta<span class="op">*</span>g</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Ne risulta un andamento oscillante sia della funzione di costo che dei valori stimati dei coefficienti. Chiaramente, l’oscillazione sarà tanto più marcata quanto minore è la dimensione dei mini-batch, e quindi quanto più si tende a SGD.</p>
<p>Gli aggiornamenti nel caso della logistic regression derivano immediatamente da quanto sopra</p>
<p><span class="math display">\[\begin{align*}
\theta_j^{(k+1)}&amp;=\theta_j^{(k)}+\eta\sum_{\mathbf{x}_i\in MB}( t_i-y_i)x_{ij}\hspace{1cm}j=1,\ldots,d\\
\theta_0^{(k+1)}&amp;=\theta_0^{(k)}+\eta\sum_{\mathbf{x}_i\in MB}(t_i-y_i)
\end{align*}\]</span></p>
<div id="a06a07f3" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mb_gd(X, t, eta <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">1000</span>, minibatch_size <span class="op">=</span> <span class="dv">5</span>):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    mb <span class="op">=</span> <span class="bu">int</span>(np.ceil(<span class="bu">float</span>(n)<span class="op">/</span>minibatch_size))</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    idx <span class="op">=</span> np.arange(<span class="dv">0</span>,n)</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    np.random.shuffle(idx)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(mb<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> idx[k<span class="op">*</span>minibatch_size:(k<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>minibatch_size]:</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>                e <span class="op">=</span> (t[i] <span class="op">-</span> sigma(theta, X[i,:]))[<span class="dv">0</span>]</span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>                g <span class="op">=</span> g <span class="op">+</span> e <span class="op">*</span> X[i,:]</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> theta <span class="op">+</span> eta <span class="op">*</span> g.reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>            theta_history.append(theta)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>            cost_history.append(cost(theta, X, t))</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q, minibatch_size</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c60c7499" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q, mbs <span class="op">=</span> mb_gd(X, t, eta <span class="op">=</span> <span class="fl">0.01</span>, epochs <span class="op">=</span> <span class="dv">10000</span>, minibatch_size <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi"</span>)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>mbs<span class="sc">}</span><span class="ss"> gradienti calcolati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  11.676 secondi
190000 passi
950000 gradienti calcolati</code></pre>
</div>
</div>
<div id="5aa9a3d3" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-34-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="274dbd6d" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>mbs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>in 190000 passi e 950000 gradienti calcolati non raggiunge un punto a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="3d76e9af" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m[<span class="op">-</span><span class="dv">1</span>],q[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-36-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="criticità" class="level4">
<h4 class="anchored" data-anchor-id="criticità">Criticità</h4>
<p>I metodi elementari di discesa del gradiente illustrati sopra non garantiscono in generale una elevata convergenza. Inoltre, il loro utilizzo pone un insieme di questioni</p>
<ul>
<li>la scelta del valore del learning rate <span class="math inline">\(\eta\)</span> può risultare difficile. Un valore troppo piccolo può comportare una convergenza eccessivamente lenta, mentre un valore troppo grande può portare ad oscillazioni intorno al minimo, o addirittura a divergenza</li>
<li>per ovviare a questa problematica è possibile utilizzare dei metodi di aggiustamento di <span class="math inline">\(\eta\)</span> nel tempo, ad esempio riducendolo secondo uno schema predefinito o quando il decremento della funzione di costo calcolata in due epoche successive risulti inferiore a una soglia data. Sia gli schemi che le soglie devono però essere predefiniti e non possono quindi adattarsi in dipendenza delle caratteristiche del dataset</li>
<li>lo stesso learning rate si applica per l’aggiornamento di tutti i coefficienti</li>
<li>in molti casi la funzione di costo, in particolare se si ha a che fare con reti neurali, risulta fortemente non convessa, caratterizzata quindi da numerosi minimi locali e da punti di sella. I metodi considerati possono avere difficoltà a uscire da situazioni di questo tipo, e in particolare dai punti di sella, spesso circondati da regioni a gradiente molto limitato.</li>
</ul>
</section>
<section id="momento" class="level4">
<h4 class="anchored" data-anchor-id="momento">Momento</h4>
<p>I metodi precedenti risultano poco efficienti in situazioni in cui la funzione di costo varia in modo molto diverso al variare della direzione considerata (ad esempio se si hanno valli che discendono lentamente e con pareti laterali ripide). In questo caso, infatti, gli algoritmi precedenti procedono molto lentamente in direzione del minimo, oscillando in modo sostanziale nella direzione trasversale ad essa: questa situazione è illustrata a sinistra nella figura sottostante.</p>
<p>Il <em>metodo del momento</em> fa riferimento ad una interpretazione fisica del metodo di ottimizzazione, in cui il processo di discesa del gradiente viene visto come lo spostamento di un corpo di massa <span class="math inline">\(m=1\)</span> che si muove sulla superficie della funzione di costo <span class="math inline">\(J(\theta)\)</span> soggetto a una forza peso <span class="math inline">\(F(\theta)=-\nabla U(\theta)\)</span>, dove <span class="math inline">\(U(\theta)=\eta h(\theta)=\eta J(\theta)\)</span> è l’energia potenziale del corpo nella posizione <span class="math inline">\(\theta\)</span> (si assume quindi che la costante fisica <span class="math inline">\(g\)</span> relativa alla forza peso <span class="math inline">\(F=-mgh\)</span> sia pari a <span class="math inline">\(\eta\)</span>). In questo modello, il valore negativo del gradiente <span class="math inline">\(-\eta J'(\theta)\)</span> è quindi pari al vettore forza (e accelerazione, in quanto <span class="math inline">\(a=\frac{F}{m}\)</span>) del corpo nel punto <span class="math inline">\(\theta\)</span>.</p>
<p>Nel metodo della discesa del gradiente, si assume che lo spostamento del corpo in un certo punto <span class="math inline">\(\theta\)</span> sia determinato dalla accelerazione calcolata nello stesso punto, e quindi dal gradiente <span class="math inline">\(J'(\theta)\)</span>, in quanto vale la regola di aggiornamento <span class="math inline">\(\theta^{(k+1)}=\theta^{(k)}-\eta J'(\theta^{(k)})\)</span>.</p>
<p>Nel metodo del momento, si fa riferimento a un modello più consistente con la realtà fisica di un corpo che si muove su una superficie soggetto alla forza peso, modello che prevede di utilizzare il concetto di velocità <span class="math inline">\(v(\theta)\)</span>. In questo modello, lo spostamento del corpo a partire da un certo punto <span class="math inline">\(\theta\)</span> è determinato dalla velocità calcolata nello stesso punto <span class="math inline">\(\theta^{(k+1)}=\theta^{(k)}+v^{(k+1)}\)</span>, dove la variazione di velocità è data dalla accelerazione <span class="math inline">\(v^{(k+1)}=v^{(k)}-\eta J'(\theta^{(k)})\)</span>.</p>
<p>Come si può osservare, si ha che</p>
<p><span class="math display">\[\begin{align*}
v^{(k+1)}&amp;=-\eta J'(\theta^{(k)})+v^{(k)}=-\eta J'(\theta^{(k)})-\eta J'(\theta^{(k-1)})+v^{(k-1)}=\cdots=-\eta\sum_{i=0}^kJ'(\theta^{(i)})+v^{(0)}\\
\theta^{(k+1)}&amp;=\theta^{(k)}+v^{(k+1)}=\theta^{(k)}-\eta\sum_{i=0}^kJ'(\theta^{(i)})+v^{(0)}
\end{align*}\]</span></p>
<p>che corrisponde all’associare lo spostamento alla somma (integrale nel caso della fisica) delle accelerazioni passate.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/momentum.jpg" class="img-fluid figure-img"></p>
<figcaption>Effetto dell’introduzione di momento nell’andamento del costo durante l’applicazione di gradient descent.</figcaption>
</figure>
</div>
<p>Il riferimento a questo modello porta l’algoritmo a tendere ad ogni passo a mantenere, almeno in parte, la direzione del passo precedente (in quanto <span class="math inline">\(v^{(k+1)}=-\eta J'(\theta^{(k)})+v^{(k)})\)</span>, premiando le direzioni che si manifestano con costanza in una sequenza di passi. Ne deriva il comportamento a destra della figura precedente, in cui l’inerzia nella direzione del minimo porta a una limitazione delle oscillazioni.</p>
<p>Si noti che ciò non avviene nella discesa del gradiente, in cui si ha <span class="math inline">\(v^{(k+1)}=-\eta J'(\theta^{(k)})\)</span>.</p>
<p>Matematicamente, l’effetto di inerzia viene ottenuto sottraendo alla velocità (vettoriale) calcolata al passo precedente la valutazione del gradiente effettuata nella corrispondente posizione. Il gradiente viene sottratto in quanto, mantenendo la corrispondenza con la meccanica di , un gradiente positivo tende a ridurre la velocità.</p>
<p>Il metodo del momento utilizza tipicamente un secondo parametro <span class="math inline">\(\gamma\)</span>, che determina la frazione di <span class="math inline">\(v^{(k)}\)</span> che permane nella definizione di <span class="math inline">\(v^{(k+1)}\)</span>, e che svolge la funzione (fisicamente) di un coefficiente di attrito. Si ottiene quindi la formulazione:</p>
<p><span class="math display">\[\begin{align*}
v^{(k+1)}&amp;=\gamma v^{(k)} -\eta\sum_{i=1}^nJ'(\theta^{(k)};x_i)\\
\theta^{(k+1)}&amp;=\theta^{(k)}+v^{(k+1)}
\end{align*}\]</span> Il metodo del momento ad ogni passo determina inizialmente il vettore di spostamento attuale, a partire da quello al passo precedente e dal gradiente di <span class="math inline">\(\theta\)</span>: il contributo relativo dei due termini è pesato dalla coppia di parametri <span class="math inline">\(\gamma\)</span> e <span class="math inline">\(\eta\)</span>. Lo spostamento calcolato viene quindi applicato al valore attuale di <span class="math inline">\(\theta\)</span> (il segno meno deriva come sempre dal fatto che stiamo assumendo di cercare un minimo locale).</p>
<p>Se il gradiente è orientato nella stessa direzione della velocità attuale, tale velocità viene incrementata, per cui l’aggiornamento di <span class="math inline">\(\theta\)</span> diviene maggiore, incrementandosi man mano che la direzione di spostamento rimane coerente con il gradiente nei punti attraversati.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(dataset_size):</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> g<span class="op">+</span>evaluate_gradient(loss_function, theta, X[k])</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> gamma<span class="op">*</span>v<span class="op">-</span>eta<span class="op">*</span>g</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta<span class="op">+</span>v</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Come si può vedere, mentre <span class="math inline">\(\theta^{(k)}=(\theta_1^{(k)},\ldots,\theta_d^{(k)})^T\)</span> è la valutazione della soluzione ottima al passo <span class="math inline">\(k\)</span>, <span class="math inline">\(v^{(k)}=(v_1^{(k)},\ldots,v_d^{(k)})^T\)</span> è l’aggiornamento applicato a tale valore per ottenere <span class="math inline">\(\theta^{(k+1)}\)</span>: possiamo vedere quindi <span class="math inline">\(v\)</span> come il vettore velocità di spostamento di <span class="math inline">\(\theta\)</span> nello spazio delle soluzioni.</p>
<p>Come già illustrato sopra, possiamo esprimere l’aggiornamento nel modo seguente, evidenziando come esso dipenda dal gradiente calcolato in tutte le posizioni precedentemente attraversate, con un effetto che va a diminuire esponenzialmente con <span class="math inline">\(\gamma\)</span> man mano che si risale nel passato. Assumendo <span class="math inline">\(v^{(0)}=0\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\theta^{(k+1)}&amp;=\theta^{(k+1)}+v^{(k+1)}= \theta^{(k)}+\gamma v^{(k)}-\eta\sum_{i=1}^nJ'(\theta^{(k)};\mathbf{x}_i)=\theta^{(k)}+\gamma^2 v^{(k-1)}-\gamma\eta\sum_{i=1}^nJ'(\theta^{(k-1)};\mathbf{x}_i) -\eta\sum_{i=1}^nJ'(\theta^{(k)};\mathbf{x}_i)\\
&amp;=\theta^{(k)}+\gamma^2 v^{(k-1)}-\eta\left(\sum_{i=1}^nJ'(\theta^{(k)};\mathbf{x}_i)+\gamma\sum_{i=1}^nJ'(\theta^{(k-1)};\mathbf{x}_i)\right)=\cdots=
\theta^{(k)}-\eta\left(\sum_{j=0}^k\gamma^j\sum_{i=1}^nJ'(\theta^{(k-j)};\mathbf{x}_i)\right)
\end{align*}\]</span></p>
<p>Gli aggiornamenti nel caso della logistic regression derivano immediatamente</p>
<p><span class="math display">\[\begin{align*}
v_j^{(k+1)}&amp;=\gamma v_j^{(k)}+\frac{\eta}{n}\sum_{i=1}^n( t_i-y_i)x_{ij}\hspace{1cm}j=1,\ldots,d\\
v_0^{(k+1)}&amp;=\gamma v_0^{(k)}+\frac{\eta}{n}\sum_{i=1}^n(t_i-y_i) \\
\theta_j^{(k+1)}&amp;=\theta_j^{(k)}+v_j^{(k+1)}\hspace{1cm}j=0,\ldots,d
\end{align*}\]</span></p>
<div id="f066fd90" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> momentum_gd(X,t, eta <span class="op">=</span> <span class="fl">0.1</span>, gamma <span class="op">=</span> <span class="fl">0.97</span>, epochs <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> gamma<span class="op">*</span>v <span class="op">-</span> eta <span class="op">*</span> gradient(theta,X,t)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> v</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e5f835c6" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q <span class="op">=</span> momentum_gd(X, t, eta <span class="op">=</span> <span class="fl">0.1</span>, gamma <span class="op">=</span> <span class="fl">0.97</span>, epochs <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi totali"</span>)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti calcolati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.353 secondi
10000 passi totali
1000000 gradienti calcolati</code></pre>
</div>
</div>
<div id="757a7084" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>low, high, step <span class="op">=</span> <span class="dv">0</span>, <span class="va">None</span>, <span class="dv">10</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, low, high, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-39-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="70abc081" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>56 passi e 5600 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="819a471f" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m[<span class="op">-</span><span class="dv">1</span>],q[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-41-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="accelerazione-del-gradiente-di-nesterov" class="level4">
<h4 class="anchored" data-anchor-id="accelerazione-del-gradiente-di-nesterov">Accelerazione del gradiente di Nesterov</h4>
<p>Nel metodo del momento, la conoscenza al passo <span class="math inline">\(k\)</span> di <span class="math inline">\(\theta^{(k)}\)</span> e di <span class="math inline">\(v^{(k)}\)</span> permette, senza calcolare il gradiente, di avere una valutazione approssimata <span class="math inline">\(\tilde{\theta}^{(k+1)}=\theta^{(k)}+\gamma v^{(k)}\)</span> di <span class="math display">\[
\theta^{(k+1)}=\theta^{(k)}+v^{(k+1)}=\theta^{(k)}+\gamma v^{(k)}-\eta\sum_{i=1}^nJ'(\theta^{(k)};\mathbf{x}_i)=\tilde{\theta}^{(k+1)}-\eta\sum_{i=1}^nJ'(\theta^{(k)};\mathbf{x}_i)
\]</span> Il metodo di Nesterov segue lo stesso approccio del metodo del momento, con la differenza che, ad ogni passo, la valutazione del gradiente viene effettuata, con un <em>look-ahead</em> approssimato, non nel punto attuale <span class="math inline">\(\theta^{(k)}\)</span> dello spazio delle soluzioni visitato ma, più o meno, nel punto successivo <span class="math inline">\(\theta^{(k+1)}\)</span> (approssimato da <span class="math inline">\(\tilde{\theta}^{(k+1)}\)</span>). In questo modo, le variazioni di <span class="math inline">\(v\)</span> (e quindi di <span class="math inline">\(\theta\)</span>) vengono anticipate rispetto a quanto avviene nel metodo del momento.</p>
<p><span class="math display">\[\begin{align*}
v^{(k+1)}&amp;=\gamma v^{(k)} +\eta\sum_{i=1}^nJ'(\tilde{\theta}^{(k)};\mathbf{x}_i)=\gamma v^{(k)} +\eta\sum_{i=1}^nJ'(\theta^{(k)}+\gamma v^{(k)};\mathbf{x}_i)\\
\theta^{(k+1)}&amp;=\theta^{(k)}+v^{(k+1)}
\end{align*}\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/nesterov.png" class="img-fluid figure-img"></p>
<figcaption>Aggiornamento per momentum e per Nesterov.</figcaption>
</figure>
</div>
<div class="sourceCode" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_epochs):</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>    theta_approx <span class="op">=</span> theta<span class="op">+</span>gamma<span class="op">*</span>v</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(dataset_size):</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> g<span class="op">+</span>evaluate_gradient(loss_function, theta_approx, X[k])</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> gamma<span class="op">*</span>v<span class="op">-</span>eta<span class="op">*</span>g</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta<span class="op">+</span>v</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div id="7ce43841" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nesterov_gd(X,t, eta <span class="op">=</span> <span class="fl">0.1</span>, gamma <span class="op">=</span> <span class="fl">0.97</span>, epochs <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> gamma<span class="op">*</span>v <span class="op">-</span> eta <span class="op">*</span> gradient(theta<span class="op">+</span>gamma<span class="op">*</span>v,X,t)</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> v</span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb55-15"><a href="#cb55-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="79a0e1bc" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q <span class="op">=</span> nesterov_gd(X, t, eta <span class="op">=</span> <span class="fl">0.1</span>, gamma <span class="op">=</span> <span class="fl">0.97</span>, epochs <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi totali"</span>)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti valutati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.369 secondi
10000 passi totali
1000000 gradienti valutati</code></pre>
</div>
</div>
<div id="af85a123" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>low, high, step <span class="op">=</span> <span class="dv">0</span>, <span class="dv">5000</span>, <span class="dv">10</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, low, high, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-44-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="fc6a6831" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>57 passi e 5700 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="40ce5017" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m[<span class="op">-</span><span class="dv">1</span>],q[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-46-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="adagrad" class="level3">
<h3 class="anchored" data-anchor-id="adagrad">Adagrad</h3>
<p>Adagrad introduce la possibilità di applicare learning rate diversi ai vari parametri da ottimizzare (dimensioni dello spazio di ricerca): in particolare, parametri per i quali si sono avuti nel corso dei passi precedenti valori elevati del gradiente avranno associati, tendenzialmente, learning rate più piccoli, in modo tale che l’aggiornamento dei prametri stessi sia limitato. Al contrario, in presenza di dimensioni (parametri) con valori del gradiente precedenti piccoli, e quindi poco modificate, il learning rate risulterà più grande, rendendo le modifiche delle componenti corrispondenti più sensibili ai valori del gradiente.</p>
<p>Come visto, nella gradient descent “base” l’aggiornamento del coefficiente <span class="math inline">\(k\)</span>-esimo è dato da</p>
<p><span class="math display">\[
\theta_j^{(k+1)}= \theta_j^{(k)}-\eta \frac{\partial J(\theta, X)}{\partial\theta_j}\Bigr\vert_{\small\theta=\theta^{(k)}}= \theta_j^{(k)}-\eta\sum_{i=1}^n\frac{\partial J(\theta;\mathbf{x}_i)}{\partial\theta_j}\Bigr\vert_{\small\theta=\theta^{(k)}}
\]</span></p>
<p>dove <span class="math inline">\(\eta\)</span> è lo stesso per tutti i coefficienti. In Adagrad, l’aggiornamento prevede un learning rate <span class="math inline">\(\eta_{j}^{(k)}\)</span>, dipendente dal coefficiente e dal passo di applicazione del metodo, definito nel modo seguente</p>
<p><span class="math display">\[
\eta_{j}^{(k)} = \frac{\eta}{\sqrt{G_{j,k}+\varepsilon}}
\]</span></p>
<p>dove <span class="math inline">\(\eta\)</span> è una costante, <span class="math inline">\(G_{j,k}=\sum_{i=0}^{k}g_{j,i}^2\)</span> è la somma dei valori <span class="math inline">\(g_{j,i}=\frac{\partial J(\theta, X)}{\partial\theta_j}\Bigr\vert_{\small\theta=\theta^{(i)}}\)</span> del gradiente relativo a <span class="math inline">\(\theta_j\)</span> per tutte le iterazioni precedenti, mentre <span class="math inline">\(\varepsilon\)</span> è una piccola costante di <em>smoothing</em> utilizzata per evitare denominatori nulli.</p>
<p>Utilizzando il formalismo introdotto, l’aggiornamento di <span class="math inline">\(\theta_j\)</span> alla <span class="math inline">\(k+1\)</span>-esima iterazione è data da</p>
<p><span class="math display">\[
\theta_j^{(k+1)}= \theta_j^{(k)}-\frac{\eta}{\sqrt{G_{j,k}+\varepsilon}}g_{j,k}
\]</span></p>
<p>Come si può vedere, il learning rate diminuisce in modo monotono al procedere delle iterazioni per tutti i coefficienti. Al tempo stesso, coefficienti con elevati valori di gradiente nel passato (quindi soggetti a variazioni significative) avranno decrementi più elevati del learning rate, che quindi tenderà più rapidamente a <span class="math inline">\(0\)</span> e a modificare poco i valori di tali cofficienti, mentre coefficienti con valori limitati (che sono variati poco fino ad ora) manterrano un learning rate più alto.</p>
<p>Dato che in ogni caso il denominatore cresce ad ogni iterazione, il learning rate continua a diminuire fino a raggiungere valori talmente piccoli da impedire un reale aggiornamento della soluzione.</p>
<div id="ad68c3d7" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adagrad(X,t, eta <span class="op">=</span> <span class="fl">0.1</span>, G <span class="op">=</span> <span class="dv">0</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, epochs <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> gradient(theta,X,t)</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>        G <span class="op">=</span> G <span class="op">+</span> g<span class="op">**</span><span class="dv">2</span></span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> g <span class="op">/</span> (np.sqrt(G) <span class="op">+</span> eps)</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb62-13"><a href="#cb62-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb62-14"><a href="#cb62-14" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb62-15"><a href="#cb62-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="e26a0d07" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q <span class="op">=</span> adagrad(X, t, eta <span class="op">=</span> <span class="fl">0.1</span>, G <span class="op">=</span> <span class="dv">0</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, epochs <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi totali"</span>)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti calcolati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.370 secondi
10000 passi totali
1000000 gradienti calcolati</code></pre>
</div>
</div>
<div id="70ec1470" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>low, high, step <span class="op">=</span> <span class="dv">0</span>, <span class="dv">5000</span>, <span class="dv">1</span></span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, low, high, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-49-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="9504fc15" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>8272 passi e 827200 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="f0b85959" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m[<span class="op">-</span><span class="dv">1</span>],q[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-51-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="rmsprop" class="level3">
<h3 class="anchored" data-anchor-id="rmsprop">RMSProp</h3>
<p>RMSprop cerca di ridurre l’aggressività di Adagrad nel ridurre il learning rate in modo monotonicamente decrescente.</p>
<p>La somma <span class="math inline">\(G_{j,k}\)</span> dei quadrati dei gradienti precedenti è sostituita da una sua versione <span class="math inline">\(\tilde G_{j,k}\)</span> che decresce in modo geometrico per mezzo di un parametro di smorzamento <span class="math inline">\(0&lt;\gamma&lt;1\)</span></p>
<p><span class="math display">\[\begin{align*}
    \tilde G_{j,k}&amp;=\gamma \tilde G_{j,k-1}+(1-\gamma)g_{j,k}^2\\
    &amp;=\gamma (\gamma \tilde G_{j,k-2}+(1-\gamma)g_{j,k-1}^2)+(1-\gamma)g_{j,k}^2=\gamma^2\tilde G_{j,k-2}+(1-\gamma)(\gamma g_{j,k-1}^2+g_{j,k}^2)\\
    &amp;=\cdots\\
    &amp;=(1-\gamma)\sum_{i=0}^k\gamma^{k-i} g_{j,i}^2
\end{align*}\]</span></p>
<p>dato che assumiamo <span class="math inline">\(\tilde G_{j,k}=0\)</span> if <span class="math inline">\(k&lt;0\)</span>.</p>
<p>Ne deriva che alla <span class="math inline">\(k+1\)</span>-esima iterazione vengono eseguite le azioni seguenti</p>
<p><span class="math display">\[\begin{align*}
g_{j, k} &amp;= \frac{\partial J(\theta)}{\partial\theta_j}\Big\rvert_{\theta=\theta^{(k)}}\\
\tilde G_{j,k} &amp; =\gamma \tilde G_{j,k-1}+(1-\gamma)g_{j,k}^2\\
\Delta_{j,k} &amp;= - \frac{\eta}{\sqrt{\tilde G_{j,k}+\varepsilon}} g_{j, k}\\
\theta_j^{(k+1)} &amp;= \theta_j^{(k)} + \Delta_{j,k}
\end{align*}\]</span></p>
<div id="42aae672" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> RMSProp(X,t, eta <span class="op">=</span> <span class="fl">0.1</span>, G_tilde <span class="op">=</span> <span class="dv">0</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, gamma <span class="op">=</span> <span class="fl">0.9</span>, epochs <span class="op">=</span> <span class="dv">10000</span>):</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> gradient(theta,X,t)</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>        G_tilde <span class="op">=</span> gamma <span class="op">*</span> G_tilde <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> gamma) <span class="op">*</span> g<span class="op">**</span><span class="dv">2</span></span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> eta <span class="op">*</span> g <span class="op">/</span> (np.sqrt(G_tilde) <span class="op">+</span> eps)</span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb69-11"><a href="#cb69-11" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb69-12"><a href="#cb69-12" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb69-13"><a href="#cb69-13" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb69-14"><a href="#cb69-14" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb69-15"><a href="#cb69-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c78ae5e5" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q <span class="op">=</span> RMSProp(X,t, eta <span class="op">=</span> <span class="fl">0.1</span>, G_tilde <span class="op">=</span> <span class="dv">0</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, gamma <span class="op">=</span> <span class="fl">0.9</span>, epochs <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi totali"</span>)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti calcolati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.394 secondi
10000 passi totali
1000000 gradienti calcolati</code></pre>
</div>
</div>
<div id="8d7be24a" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m[<span class="op">-</span><span class="dv">1</span>],q[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-54-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="ed7fd078" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">3e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>49 passi e 4900 gradienti calcolati per arrivare a distanza minore o uguale di 0.3 dall'ottimo</code></pre>
</div>
</div>
</section>
<section id="adadelta" class="level3">
<h3 class="anchored" data-anchor-id="adadelta">Adadelta</h3>
<p>Adadelta è una estensione di RMSprop in cui non è presente un valore <span class="math inline">\(\eta\)</span> arbitrariamente definito: questo parametro è sostituito da una somma smorzata, mediante un coefficiente <span class="math inline">\(0&lt;\gamma&lt;1\)</span>, dei quadrati degli aggiornamenti precedenti. Inoltre, al fine di limitare una eccessiva diminuzione del learning rate, non vengono accumulati tutti i gradienti passati attraverso la somma dei loro quadrati, ma viene invece applicato anche qui uno smorzamento con lo stesso coefficiente <span class="math inline">\(0&lt;\gamma&lt;1\)</span>.</p>
<p>L’aggiornamento alla <span class="math inline">\(k+1\)</span>-esima iterazione è quindi definito come:</p>
<p><span class="math display">\[\begin{align*}
g_{j, k} &amp;= \frac{\partial J(\theta)}{\partial\theta_j}\Big\rvert_{\theta^{(k)}}\\
\tilde G_{j,k} &amp; =\gamma \tilde G_{j,k-1}+(1-\gamma)g_{j,k}^2\\
\Delta_{j,k} &amp;= - \frac{\sqrt{\overline G_{j,k-1}+\varepsilon}}{\sqrt{\tilde G_{j,k}+\varepsilon}} g_{j, k}\\
\overline G_{j,k} &amp; =\gamma \overline G_{j,k-1}+(1-\gamma)\Delta_{j,k}^2\\
\theta_j^{(k+1)} &amp;= \theta_j^{(k)} + \Delta_{j,k}
\end{align*}\]</span></p>
<div id="5ce01815" class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adadelta(X,t, G_tilde <span class="op">=</span> <span class="dv">0</span>, G_over <span class="op">=</span> <span class="dv">0</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, gamma <span class="op">=</span> <span class="fl">0.9</span>, epochs <span class="op">=</span> <span class="dv">10000</span>):</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> gradient(theta,X,t)</span>
<span id="cb75-7"><a href="#cb75-7" aria-hidden="true" tabindex="-1"></a>        G_tilde <span class="op">=</span> gamma <span class="op">*</span> G_tilde <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> gamma)<span class="op">*</span>g<span class="op">**</span><span class="dv">2</span></span>
<span id="cb75-8"><a href="#cb75-8" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> <span class="op">-</span> np.sqrt(G_over <span class="op">+</span> eps) <span class="op">/</span> np.sqrt(G_tilde <span class="op">+</span> eps) <span class="op">*</span> g</span>
<span id="cb75-9"><a href="#cb75-9" aria-hidden="true" tabindex="-1"></a>        G_over <span class="op">=</span> gamma <span class="op">*</span> G_over <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> gamma) <span class="op">*</span> delta<span class="op">**</span><span class="dv">2</span></span>
<span id="cb75-10"><a href="#cb75-10" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> delta</span>
<span id="cb75-11"><a href="#cb75-11" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb75-12"><a href="#cb75-12" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb75-13"><a href="#cb75-13" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb75-14"><a href="#cb75-14" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb75-15"><a href="#cb75-15" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb75-16"><a href="#cb75-16" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb75-17"><a href="#cb75-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="6a632646" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q <span class="op">=</span> adadelta(X,t, G_tilde <span class="op">=</span> <span class="dv">0</span>, G_over <span class="op">=</span> <span class="dv">0</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, gamma <span class="op">=</span> <span class="fl">0.9</span>, epochs <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi totali"</span>)</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti valutati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.443 secondi
10000 passi totali
1000000 gradienti valutati</code></pre>
</div>
</div>
<div id="e08535bd" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>low, high, step <span class="op">=</span> <span class="dv">0</span>, <span class="dv">5000</span>, <span class="dv">10</span></span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, low, high, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-58-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="7adbba94" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>7075 passi e 707500 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="81187b95" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>plot_ds(data,m[<span class="op">-</span><span class="dv">1</span>],q[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-60-output-1.png" width="954" height="497" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="adam" class="level3">
<h3 class="anchored" data-anchor-id="adam">Adam</h3>
<p>Oltre a memorizzare la somma smorzata <span class="math inline">\(\tilde G_{j,k}\)</span> dei quadrati dei gradienti precedenti, come in Adadelta e RMSprop, Adam mantiene anche una somma smorzata <span class="math inline">\(\tilde H_{j,k}\)</span> dei gradienti precedenti <span class="math inline">\(g_{j,k}\)</span>, utilizzata in sostituzione del gradiente <span class="math inline">\(g_{j,k}\)</span> nell’aggiornamento effettuato ad gni iterazione.</p>
<p><span class="math display">\[\begin{align*}
\tilde G_{j,k} &amp;= \gamma \tilde G_{j,k-1} + (1 - \gamma) g_{j,k}^2\\
\tilde H_{j,k} &amp;= \beta \tilde H_{j,k-1} + (1 - \beta) g_{j,k}
\end{align*}\]</span></p>
<p>Dato che si assume che <span class="math inline">\(\tilde H_{j,k}=\tilde G_{j,k}=0\)</span> se <span class="math inline">\(k&lt;0\)</span> e i valori <span class="math inline">\(\gamma, \beta\)</span> sono normalmente prossimi a <span class="math inline">\(1\)</span>, il metodo manifesta la tendenza (bias) a fornire valori piccoli di <span class="math inline">\(\tilde H_{j,k}\)</span> e <span class="math inline">\(\tilde G_{j,k}\)</span>, soprattutto nelle iterazioni iniziali.</p>
<p>Per correggere questo comportamento viene applicata una correzione del bias:</p>
<p><span class="math display">\[\begin{align*}
\hat{G}_{j,k} &amp;= \frac{\tilde G_{j,k}}{1 - \gamma^k}\\
\hat{H}_{j,k} &amp;= \frac{\tilde H_{j,k}}{1 - \beta^k}
\end{align*}\]</span></p>
<p>I parametri sono aggiornati come visto in Adadelta e RMSprop, per cui risulta ad ogni iterazione:</p>
<p><span class="math display">\[\begin{align*}
g_{j, k} &amp;= \frac{\partial J(\theta)}{\partial\theta_j}\Big\rvert_{\theta=\theta^{(k)}}\\
\tilde G_{j,k} &amp; =\gamma \tilde G_{j,k-1}+(1-\gamma)g_{j,k}^2\\
\tilde H_{j,k} &amp; =\beta \tilde H_{j,k-1}+(1-\beta)g_{j,k}\\
\hat{G}_{j,k} &amp;= \frac{\tilde G_{j,k}}{1 - \gamma^k}\\
\hat{H}_{j,k} &amp;= \frac{\tilde H_{j,k}}{1 - \beta^k}\\
\Delta_{j,k} &amp;= - \frac{\eta}{\sqrt{\hat G_{j,k}+\varepsilon}} \hat H_{j, k}\\
\theta_j^{(k+1)} &amp;= \theta_j^{(k)} + \Delta_{j,k}
\end{align*}\]</span></p>
<div id="55e294b1" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Adam(X,t, eta <span class="op">=</span> <span class="fl">0.1</span>, gamma <span class="op">=</span> <span class="fl">0.999</span>, beta <span class="op">=</span> <span class="fl">0.9</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, epochs <span class="op">=</span> <span class="dv">10000</span>):</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>    G_tilde <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>    H_tilde <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> gradient(theta,X,t)</span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>        G_tilde <span class="op">=</span> gamma <span class="op">*</span> G_tilde <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> gamma)<span class="op">*</span>g<span class="op">**</span><span class="dv">2</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>        H_tilde <span class="op">=</span> beta <span class="op">*</span> H_tilde <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta)<span class="op">*</span>g</span>
<span id="cb82-11"><a href="#cb82-11" aria-hidden="true" tabindex="-1"></a>        G_hat <span class="op">=</span> G_tilde<span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span> gamma<span class="op">**</span>(k<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb82-12"><a href="#cb82-12" aria-hidden="true" tabindex="-1"></a>        H_hat <span class="op">=</span> H_tilde<span class="op">/</span>(<span class="dv">1</span> <span class="op">-</span> beta<span class="op">**</span>(k<span class="op">+</span><span class="dv">1</span>))</span>
<span id="cb82-13"><a href="#cb82-13" aria-hidden="true" tabindex="-1"></a>        delta <span class="op">=</span> <span class="op">-</span> eta <span class="op">*</span> H_hat<span class="op">/</span>(np.sqrt(G_hat)<span class="op">+</span> eps)</span>
<span id="cb82-14"><a href="#cb82-14" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> delta</span>
<span id="cb82-15"><a href="#cb82-15" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb82-16"><a href="#cb82-16" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb82-17"><a href="#cb82-17" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb82-18"><a href="#cb82-18" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb82-19"><a href="#cb82-19" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb82-20"><a href="#cb82-20" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb82-21"><a href="#cb82-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="6e78522e" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q <span class="op">=</span> Adam(X,t, eta <span class="op">=</span> <span class="fl">0.1</span>, gamma <span class="op">=</span> <span class="fl">0.999</span>, beta <span class="op">=</span> <span class="fl">0.9</span>, eps <span class="op">=</span> <span class="fl">1e-8</span>, epochs <span class="op">=</span> <span class="dv">10000</span>)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi totali"</span>)</span>
<span id="cb83-5"><a href="#cb83-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti valutati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.445 secondi
10000 passi totali
1000000 gradienti valutati</code></pre>
</div>
</div>
<div id="3dd2b611" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>low, high, step <span class="op">=</span> <span class="dv">0</span>, <span class="dv">5000</span>, <span class="dv">10</span></span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q, low, high, step)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-63-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="9efeae2a" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>254 passi e 25400 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
</section>
<section id="metodi-del-secondo-ordine" class="level3">
<h3 class="anchored" data-anchor-id="metodi-del-secondo-ordine">Metodi del secondo ordine</h3>
<p>La ricerca di un punto di massimo (o minimo) può anche essere effettuata in termini di ricerca di punti in cui la derivata prima (o il gradiente, in generale) si annullano, applicando uno dei metodi iterativi standard per la ricerca degli zeri di una funzione.</p>
<p>Un tipico metodo utilizzato in questo ambito è quello di Newton-Raphson, in cui (considerando una funzione univariata) viene applicato, ad ogni iterazione, l’aggiornamento</p>
<p><span class="math display">\[
x_{i+1}=x_{i}-\frac{f(x_{i})}{f'(x_{i})}
\]</span></p>
<p>A ogni iterazione, l’algoritmo approssima <span class="math inline">\(f\)</span> per mezzo di una retta tangente a <span class="math inline">\(f\)</span> in <span class="math inline">\((x_i,f(x_{i}))\)</span>, e definisce <span class="math inline">\(x_{i+1}\)</span> come il valore in cui tale retta interseca l’asse <span class="math inline">\(x\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="assets/newton.png" class="img-fluid figure-img"></p>
<figcaption>Newton-Raphson.</figcaption>
</figure>
</div>
<p>Nel caso in cui si cercano punti di massimo o minimo, l’iterazione evidentemente diviene</p>
<p><span class="math display">\[
x_{i+1}=x_{i}-\frac{f'(x_{i})}{f''(x_{i})}
\]</span></p>
<p>Nel caso di funzioni a più variabili, la derivata prima è sostituita dal gradiente <span class="math inline">\(\nabla f\)</span>, mentre la derivata seconda corrisponde alla matrice <em>Hessiana</em> <span class="math inline">\(H\)</span>, definita come</p>
<p><span class="math display">\[
H_{ij}(f)=\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}
\]</span></p>
<p>L’aggiornamento ad ogni iterazione diviene quindi</p>
<p><span class="math display">\[
\mathbf{x}^{(i+1)}=\mathbf{x}^{(i)}-\big(H(f)^{-1}\nabla f\big)\big|_{ \mathbf{x}_{(i)}}
\]</span></p>
<div id="2154d642" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> theta_history[<span class="dv">1</span>]</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a>c1 <span class="op">=</span> sigma(theta,X)</span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> np.zeros([<span class="dv">3</span>,<span class="dv">3</span>])</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t.shape[<span class="dv">0</span>]):</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a>    m<span class="op">=</span>X[i,:].reshape(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>    h <span class="op">=</span> h<span class="op">+</span>np.dot(m.T,m)<span class="op">*</span>c1[i]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>c1[i])</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>h <span class="op">=</span> h<span class="op">/</span>t.shape[<span class="dv">0</span>]</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>g<span class="op">=</span>gradient(theta,X,t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="51ed22d8" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>g.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>(3, 1)</code></pre>
</div>
</div>
<div id="54894cad" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>np.dot(np.linalg.inv(h),g)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>array([[-2.66848161],
       [-0.37416659],
       [ 0.35083743]])</code></pre>
</div>
</div>
<div id="19f25416" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient(theta, X, t):</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="op">-</span>np.dot(X.T, (t<span class="op">-</span>sigma(theta, X))) <span class="op">/</span> <span class="bu">len</span>(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="b563d3f3" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> newton(X,t, epochs <span class="op">=</span> <span class="dv">100</span>):</span>
<span id="cb94-2"><a href="#cb94-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb94-3"><a href="#cb94-3" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb94-4"><a href="#cb94-4" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb94-5"><a href="#cb94-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb94-6"><a href="#cb94-6" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> gradient(theta,X,t)</span>
<span id="cb94-7"><a href="#cb94-7" aria-hidden="true" tabindex="-1"></a>        c1 <span class="op">=</span> sigma(theta,X)</span>
<span id="cb94-8"><a href="#cb94-8" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> np.zeros([nfeatures<span class="op">+</span><span class="dv">1</span>,nfeatures<span class="op">+</span><span class="dv">1</span>])</span>
<span id="cb94-9"><a href="#cb94-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t.shape[<span class="dv">0</span>]):</span>
<span id="cb94-10"><a href="#cb94-10" aria-hidden="true" tabindex="-1"></a>            m<span class="op">=</span>X[i,:].reshape(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb94-11"><a href="#cb94-11" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> h<span class="op">+</span>np.dot(m.T,m)<span class="op">*</span>c1[i]<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>c1[i])</span>
<span id="cb94-12"><a href="#cb94-12" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> h<span class="op">/</span>t.shape[<span class="dv">0</span>]</span>
<span id="cb94-13"><a href="#cb94-13" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> np.dot(np.linalg.inv(h),g)</span>
<span id="cb94-14"><a href="#cb94-14" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb94-15"><a href="#cb94-15" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb94-16"><a href="#cb94-16" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb94-17"><a href="#cb94-17" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb94-18"><a href="#cb94-18" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb94-19"><a href="#cb94-19" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb94-20"><a href="#cb94-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="0e3d0f01" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a>start_time <span class="op">=</span> time.time()</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a>cost_history, m, q <span class="op">=</span> newton(X,t, epochs <span class="op">=</span> <span class="dv">100</span>)</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tempo di esecuzione: </span><span class="sc">{</span>time<span class="sc">.</span>time()<span class="op">-</span>start_time<span class="sc">: 4.3f}</span><span class="ss"> secondi"</span>)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="sc">}</span><span class="ss"> passi totali"</span>)</span>
<span id="cb95-5"><a href="#cb95-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span><span class="bu">len</span>(m)<span class="op">*</span>n<span class="sc">}</span><span class="ss"> gradienti valutati"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tempo di esecuzione:  0.084 secondi
100 passi totali
10000 gradienti valutati</code></pre>
</div>
</div>
<div id="af82da6e" class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>plot_all(cost_history, m, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="loss_files/figure-html/cell-71-output-1.png" width="1288" height="422" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="2f6fe4ce" class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>near_opt(m, min_dist<span class="op">=</span><span class="fl">1e-1</span>, ge_per_step<span class="op">=</span>n)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>3 passi e 300 gradienti calcolati per arrivare a distanza minore o uguale di 0.1 dall'ottimo</code></pre>
</div>
</div>
<div id="b89894e4" class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> newton(X,t, epochs <span class="op">=</span> <span class="dv">100</span>):</span>
<span id="cb100-2"><a href="#cb100-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.zeros(nfeatures<span class="op">+</span><span class="dv">1</span>).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb100-3"><a href="#cb100-3" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> []</span>
<span id="cb100-4"><a href="#cb100-4" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> []</span>
<span id="cb100-5"><a href="#cb100-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(epochs):           </span>
<span id="cb100-6"><a href="#cb100-6" aria-hidden="true" tabindex="-1"></a>        g <span class="op">=</span> gradient(theta,X,t)</span>
<span id="cb100-7"><a href="#cb100-7" aria-hidden="true" tabindex="-1"></a>        c1 <span class="op">=</span> sigma(theta,X)</span>
<span id="cb100-8"><a href="#cb100-8" aria-hidden="true" tabindex="-1"></a>        c2 <span class="op">=</span> c1<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>c1)</span>
<span id="cb100-9"><a href="#cb100-9" aria-hidden="true" tabindex="-1"></a>        h0<span class="op">=</span>np.einsum(<span class="st">'ij,ik-&gt;ijk'</span>,X,X)</span>
<span id="cb100-10"><a href="#cb100-10" aria-hidden="true" tabindex="-1"></a>        h<span class="op">=</span>np.einsum(<span class="st">'ijk, iq-&gt; jk'</span>,h0,c2)<span class="op">/</span>t.shape[<span class="dv">0</span>]</span>
<span id="cb100-11"><a href="#cb100-11" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">-</span> np.dot(np.linalg.inv(h),g)</span>
<span id="cb100-12"><a href="#cb100-12" aria-hidden="true" tabindex="-1"></a>        theta_history.append(theta)</span>
<span id="cb100-13"><a href="#cb100-13" aria-hidden="true" tabindex="-1"></a>        cost_history.append(cost(theta, X, t))</span>
<span id="cb100-14"><a href="#cb100-14" aria-hidden="true" tabindex="-1"></a>    theta_history <span class="op">=</span> np.array(theta_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">3</span>)</span>
<span id="cb100-15"><a href="#cb100-15" aria-hidden="true" tabindex="-1"></a>    cost_history <span class="op">=</span> np.array(cost_history).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb100-16"><a href="#cb100-16" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">1</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb100-17"><a href="#cb100-17" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> <span class="op">-</span>theta_history[:,<span class="dv">0</span>]<span class="op">/</span>theta_history[:,<span class="dv">2</span>]</span>
<span id="cb100-18"><a href="#cb100-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cost_history, m, q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>